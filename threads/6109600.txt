Thread ID: 6109600
User 497513 (Parent Post) says:
<document version="2.0"><paragraph>I'd like to walk through some details about the density estimation below.</paragraph><paragraph>1. At the high level, we are interested in density estimation for different reasons. We can, for example, use the likelihood to make decisions quantitatively (likelihood classifier); inquire about the confidence of certain hypotheses (p-value); conclude the correlation/independence of several random variables, etc. It is the cornerstone of all statistical &amp; machine-learning practices. </paragraph><paragraph>In real-world problems, the true underlying density is never known. (In fact, the concept of 'randomness' has been challenged in some metaphysical contexts. As applied science/engineering practitioners, we wouldn't go that far but accept randomness as a powerful tool for characterizing reality within the scope of human capability.) Inevitably, we need to estimate the density out of the given observations.</paragraph><paragraph>Keep in mind in the context of the density estimation, we are given data, and to produce a <bold>function</bold>. Now let's come back to the hw question. </paragraph><paragraph>2. In many scriptures, you will see the notation as $P(X = x)$. The capital letter $X$ denotes a random variable, which is not certain due to the randomness. The low-case $x$ represents an observation or, more formally, a realization of the $X$. Similar notation can be $\mathbb E_X[f(X=x)], P_X(x)$, etc.</paragraph><paragraph>In the hw3 Q2, we are given a set of observations for two random variables $\{acc, amygdala\}$. To estimate the density, we first make the assumption that the realizations of those random variables are continuous. </paragraph><paragraph>3. With KDE, we derive a continuous function based on the given realizations, </paragraph><paragraph>$f(x) = \frac1m \sum_{i=1}^m \frac1{h^n} K(\frac{x-x^i}{h})$</paragraph><paragraph>please note in the expression that, $x$ is a variable, $x^i$ is the given observation, a fixed value.</paragraph><paragraph>Ideally, plotting a continuous density function requires specifying all possible values of the support, which is not practical. In digital computer systems, we plot the curve by sampling a set of points in the support.</paragraph><paragraph>For example, imagine we are to plot a parabola $y = x^2, x\in \mathbb R$, we will consider the following snippet in python</paragraph><snippet language="py" runnable="true" line-numbers="true"><snippet-file id="code">import numpy as np
import matplotlib.pyplot as plt
x = np.linspace(-5,5, 101)
plt.plot(x**2)</snippet-file></snippet><paragraph>We only specified 101 points in this case. Yet the computer system will produce a smooth due to the internal interpolation mechanism.</paragraph><paragraph>With the same idea, to plot the KDE function we derived from the data points, we also need to specify a set of values of the $x$, and leave the rest of the job to the computer.</paragraph><paragraph>When plotting a 2D function $f(x), x\in \mathbb R^2$, the values of $x$ lie in a 2D plane. Therefore, we need to define a set of meshgrid-like values, which mimic the plane we are interested in. </paragraph><paragraph>This is the same idea as the rasterized image. (ref: https://en.wikipedia.org/wiki/Rasterisation) </paragraph><figure><image src="https://static.us.edusercontent.com/files/qJuvtKidBmPBWtyyQlmEgMt5" width="68" height="61"/></figure><paragraph>In the fish image example, by referring to a 2d density function, we can think of the image with each pixel selected as a certain value in the support, either 1 or 0.</paragraph><paragraph>3. The independence of two random variables is one of the core interests in statistics. In applications, it can conclude the correlation of two separate factors (like the one we have in hw), serve as criteria to evaluate how a generative model performs given the training data, etc.</paragraph><paragraph>Some students from previous classes mentioned the $\chi^2$ test or Fisher's test. Those methods tackle the categorical data with discrete distribution and don't work in our problem.</paragraph><paragraph>In this problem, we have suggested using the definition for investigating independence.</paragraph><paragraph>$X \perp Y, \quad \text{i.f.f.} \quad P(X = x, Y = y) = P(X = x)P(Y = y), \forall x, y$</paragraph><paragraph>Please note the above conclusion requires the equation $P(X = x, Y = y) = P(X = x)P(Y = y)$ holds for all possible $x, y$. For illustrative purposes, we can simplify the problem by evaluating only a set of points in (x, y), and further compare $P(X = x, Y = y)$ and $P(X = x)P(Y = y)$ with visual checking.</paragraph><paragraph>4. If we want a more rigorous investigation of independence, we need to use the statistical distance. I laid out a brief survey below for your reference. You will find them in many theoretical papers in statistics/machine learning.</paragraph><paragraph>There are different choices of the distance metric, many of which can be categorized into two families, </paragraph><paragraph><bold>$f$-divergence</bold> is a function that measures the difference between two probability distributions $P$ and $Q$. For a convex function $f$ such that $f(1) = 0$, the $f$-divergence is defined as $$D_f(P|Q) = \int_{\Omega} f(\frac{dP}{dQ})$$ If both $P$ and $Q$ are absolutely continuous with respect to a reference distribution $\mu$ on $\Omega$, their density functions follow $p \cdot d\mu= dP, q \cdot d\mu=dQ$, the $f$-divergence becomes $$D_f(P|Q) = \int_{\Omega} f(\frac{p(x)}{q(x)})~q(x)~d\mu(x)$$ </paragraph><paragraph>With different choices of function $f$, $f$-divergence leads to difference special cases  </paragraph><paragraph>KL-divergence:  $f(t) = t \log t$</paragraph><paragraph>Pearson $\chi^2$ -divergence:  $f(t) = (t-1)^2$</paragraph><paragraph>Total Variation Distance:  $f(t)= \frac12 |t-1|$</paragraph><paragraph>Jensen-Shannon Distance:  $f(t) = (t+1)\log(\frac2{t+1}) + t\log t$ </paragraph><paragraph><bold>Probability Metrics (IPMs)</bold> is another popular family of distance measures. </paragraph><paragraph>$\gamma_{\mathscr{F}}(p, q) := \sup_{f\in \mathscr{F}} \int_{\Omega} |fdp - fdq |$ </paragraph><paragraph>where $\mathscr{F}$ is a class of functions that are real-valued and bounded on $\Omega$ </paragraph><paragraph>Different popular metrics can be obtained by choosing different $\mathscr{F}$. </paragraph><paragraph>Maximum Mean Distance: $\mathcal{H}$ is a reproducing kernel space (RKHS)} $\mathscr{F} = {f: |f|_<italic>{\mathcal{H}} = 1 }$</italic></paragraph><paragraph><italic>Wasserstein Distance:  $\mathscr{F} = {f: |f|_<italic>L \leq 1}$</italic></italic></paragraph><paragraph><italic><italic>Total Variation Distance:  $\mathscr{F} = |f: |f|_</italic>{\infty} \leq 1 |$</italic></paragraph><paragraph><italic>Dudley Metric:  $\mathscr{F} = {f: |f|_</italic>{BL} \leq 1}$</paragraph><paragraph>Kolmogorov Distance : $\mathscr{F} = { \mathbb{1}_{(-\infty, t]}: t\in \mathbb{R}^d }$ </paragraph><paragraph/><paragraph>----------------------------------</paragraph><paragraph>A question from previous session,</paragraph><paragraph>Q: Hi TAs, is it correct to use correlation method (eg. pearson correlation) for the independence question? </paragraph><paragraph>A: Technically, one can use correlation methods to conclude the independence between two random variables. However, it only works well for univariate cases. Note the correlation coefficient only applies to univariate random variables.</paragraph><paragraph>One can still follow the general correlation approach by using additional procedures to summarize the test statistics for multivariate cases, but the analysis becomes much more complicated. Therefore, it is rare to use correlation methods in practice.</paragraph><paragraph>(As I mentioned above, the testing for independence is the core study in statistics and requires more academic background beyond the scope of our course. Therefore, we will not expand the discussion here.)</paragraph><paragraph>ps: If you are comfortable with related statics methods and choose to use the correlation method, please specify the formulation of your test statistic in your report.</paragraph></document>

