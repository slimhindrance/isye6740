Thread ID: 6003537
User 1316216 (Parent Post) says:
<document version="2.0"><paragraph>Hello all,</paragraph><paragraph>I'm struggling a little with making sure I understand the math behind ISOMAP.  Would anyone be willing to read my answer to 1.4 and let me know if I have it down or I'm missing something?  It starts to get confusing in the later steps when we are talking about centering.  This is all fascinating stuff but man it can be intimidating.</paragraph><paragraph>Related - is this the most appropriate way to ask if I'm on the right track for a problem?</paragraph><paragraph>I'm not sure if the latex will paste nicely here or not, but I'm writing this all in a markdown cell in a Jupyter notebook if that makes it easier to read.</paragraph><paragraph>Thank you all for your help!</paragraph><paragraph>David</paragraph><paragraph/><heading level="3">4.</heading><paragraph>The lectures give us 3 key ideas for ISOMAP (all quotes are directly from lecture slides):</paragraph><paragraph>Overall: "...produce low dimensional representation which preserves "walking-distance" over the data cloud (manifold)"</paragraph><paragraph>First: <bold>"Find neighbors N(i) of each data point, $x^i$, within distance $\epsilon$ and let A be the adjacency matrix recording neighbor Euclidean distance"</bold></paragraph><paragraph>Second: <bold>"Find shortest path distance matrix D between each pairs of points, $x^i$ and $x^j$, based on A"</bold></paragraph><paragraph>Third: <bold>"Find low dimensional representation which preserves the distances information in D"</bold></paragraph><paragraph>For the first idea we are constructing an adjacency matrix $A$, where given points data points $x^i$ and $x^j$ and a positive constant $\epsilon$ we define $A$ as: $$ A_{ij} = \begin{cases} ||x^i - x^j|| &amp; \text{if } ||x^i - x^j|| \le \epsilon \ 0 &amp; \text{otherwise } \end{cases} $$ We can see by design this matrix will be symmetric as $||x^i-x^j|| = ||x^j-x^i||$. Overall, we think of $A$ as a matrix where nonzero entries represent "closeness" between the data points of associated row and column indecies. The closer these entries are to zero, without being equal to zero, then the closer the data points are in the manifold.</paragraph><paragraph>For the second idea, we need to utilize A in such a way that between each pair of points $x^i$ and $x^j$ we find a path (i.e. a way to hop between connecting data points) in the data cloud that minimizes the distance across those hops.</paragraph><paragraph>If we are trying to find a path between $x^i$ and $x^j$, if $A_{ij} \neq 0$ we are done as they are already within the epsilon threshold and by the triangle inequality there is no shorter path. If $A_{ij} = 0$, we must use some algorithm to find intermediate points (i.e. "the path") such that the path is minimized, where for each successive intermediate points $x^a$, $x^b$ along our path $A_{ab} \neq 0$. In other words, any pair of successive intermediate points must have distance less than the $\epsilon$ threshold in addition to our requirement to minimize path length. Once we have found this shortest distance path, it is stored in our matrix D.</paragraph><paragraph>The shortest path distance matrix D is known as the graph distance matrix, and per lecture can be computed with the Floyd-Warshall algorithm or m(m-1)/2 applications of Dijkstra's algorithm.</paragraph><paragraph>For the third idea, we are essentially trying to unfold our high-dimensional manifold into euclidean space. In other words, we are trying to flatten the manifold out in such a way that the distance is preserved between points, reducing dimensionality. The algorithm we use is known as MDS (multi-dimensional scaling).</paragraph><paragraph>Given the distance matrix $D$ in a higher dimensional space associated with our manifold, we want to map points in $D$ to a lower k-dimensional euclidean space with coordinates $z^i = (z_1^i,...,z_k^i)$ utilizing the euclidean distance. Furthermore, the distance between points must be preserved as much as possible, i.e. given $D = (d_{ij})$, we must find vectors $z^1,...,z^m \in \mathbb{R}^k$ where $d_{ij}$ approximates $||z^i - z^j||$ as closely as possible.</paragraph><paragraph>Overall, these 3 key ideas will allow us to then complete the remainder of the ISOMAP steps, where we can take the k leading eigenvalues and eigenvectors from the centered matrix $D^2$ to form the dimensionally reduced representation of the graph distance matrix.</paragraph></document>

Answer from User 799310:
While we can't verify correctness here, I definitely think you are on the right track. It looks like you were able to successfully distill some of the main ideas from the lecture.

In regards to the centering, it can be a bit confusing as you mention. The key idea is that by centering the squared distance matrix D2, we recover the Gram matrix G, which is then used in the eigen decomposition to obtain the low dimensional embedding. Slides 14-16 cover it mostly.

 

  Comment from User 1316216:
  Thank you David!

Comment from User 1316216:
PS Mark I just saw your post with Notes on ISOMAP, I will go review it thoroughly.

