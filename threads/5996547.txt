Thread ID: 5996547
User 134876 (Parent Post) says:
<document version="2.0"><paragraph>Please ask all questions related to HW2 Q1 here.</paragraph><list style="bullet"><list-item><paragraph>For 1.6, if you are using an example, you may use a PCA package.</paragraph></list-item></list></document>

Comment from User 1316216:
Hello!  Can you please clarify what is being asked in 1.4?  More specifically, is the ask that we more fully explain each of these three points from the lecture?  Thanks!



  Reply from User 213547:
  yes. you expand the 3 points

Comment from User 1497720:
For 1.3 would the identity matrix count as a valid U for the decomposition or did you only want non-I solutions? 

  Reply from User 134876:
  Prof X confirmed that the identify matrix can be used for one of your decompositions, and the other should be a non-identity matrix solution.

Comment from User 1512055:
I'm a little bit confused about exercise 3. I found two eigenvalues and their corresponding eigenvectors, which give me the first eigenvalue decomposition. What about the second one? Can someone give me a hint about how to find it? 



  Reply from User 300188:
  If I multiply an eigenvector by a scalar is it still a valid eigenvector for that particular eigenvalue? 

Comment from User 121859:
Trying to understand what needs to be delivered on 1.2. Does a simple text explaining how to get the second and third directions would suffice?   

  Reply from User 300188:
  Sure. It's not a proof in this question, just make sure to explain the steps to get the second and third directions (though not explicitly required, hopefully you include some type of mathematical formulation/expression in your explanation since it's more concise and easier to get ideas across). 



Comment from User 962052:
Why do we divide the covariance matrix by m and not m-1? I know the formula for sample covariance would divide by m-1, but just curious of the intuition here.

  Reply from User 979414:
  This is a great question. 

In the grand scheme of things, m is usually a sufficiently large number, hence dividing by m keeps the math / formula simpler. 

Comment from User 1229905:
Not a question, but found this post which does an awesome job of explaining PCA, and thought I might share: Making sense of principal component analysis, eigenvectors & eigenvalues



  Reply from User 1080381:
  Thank you for this!!

Comment from User 1512061:
Hi, I just wanted to double check for 1.1, are the proof steps from Lecture 4 Slides 17-19 (starting with title "Solving the PCA problem")? 

And for 1.6, does this require a mathematical explanation or can we just explain conceptually?

  Reply from User 497513:
  1. you can use any lecture material you find approperiate.

2. Yes.

Comment from User 1128057:
Is question 1.5 asking us to explain how to decide the optimal k for our dataset ? Thank you.

  Reply from User 213547:
  optimal k for any dataset

Comment from User 1489130:
Q1.1 reads, "Please prove the first principle component direction $v$ corresponds to the largest eigenvector of the sample covariance matrix,"  which doesn't quite make sense to me.  The optimization problem constrains any eigenvectors' magnitude to $||w||\leq1$.  Therefore, the largest magnitude of any eigenvector is 1, but any vector can be normalized to unit length, $||w||=1$.  Since larger magnitudes will help maximize the objective function, I'd assume that all eigenvectors would end up having unit length, and there would be no way to distinguish one eigenvector from another on the basis of "largeness".  

Additionally, the bottom of Lecture 4 Slide 18 (shown below) states, "The problem becomes finding the largest eigenvalue of $C$."

Using the eigenvalue to distinguish one eigenvector from another does make sense to me.  Also, it's my understanding that when you form a Lagrangian dual problem, you introduce dual variables, which in this case would be $\lambda$, and those become your new decision variables that you solve for.

Given my assumption from earlier that all eigenvectors would end up having unit length and how the new decision variable in the dual problem is $\lambda$, should Q1.1, perhaps read, "Please prove the first principle component direction $v$ corresponds to the largest eigenvalue of the sample covariance matrix," instead? 

  Reply from User 213547:
  You need to show that maximizing the variance along a unit vector "v" is equivalent to finding the eigenvector associated with the largest eigenvalue of the covariance matrix; 



Comment from User 1323477:
Any tips for 1.5? I can't find the information anywhere, especially not in lecture

  Reply from User 215365:
  Section 14.5 (Principal Components, Curves, and Surfaces) of Elements of Statistical Learning could help. This reference can be found in Canvas under Modules. 

Comment from User 307048:
For question 1.2, is it not enough to state that the eigenvectors of covariance matrix C correspond to each principal component? Or is there some other derivation I'm missing? In the lecture it just discusses how the second, third, etc. eigenvectors correspond to the second, third, etc. largest eigenvalues.

  Reply from User 300188:
  What does that process look like iteratively? Try to show that. What problem are you solving exactly to find the second and third principal components respectively and what does it look like? 



Comment from User 780084:
I am still a bit confused by 1.5, am i required to write code for an example or i can jsut explain this conceptually ? 

  Reply from User 300188:
  Conceptually is fine. 

