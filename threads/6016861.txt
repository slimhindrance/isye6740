Thread ID: 6016861
User 497513 (Parent Post) says:
<document version="2.0"><paragraph>This post extends the discussion of Kmeans and aims to encourage students to read the academic literature. You can cross-reference three sources: the lecture slides, the   [<link href="https://en.wikipedia.org/wiki/K-means_clustering">Wikipage</link>], and the original publication paper (attached below) to digest the literature better.</paragraph><paragraph>Let's begin with the definition of the K-means algorithm on Wikipedia: a method of vector quantization.</paragraph><paragraph>The vector quantization problem is one of the main interests in electrical engineering (EE)/signal processing. A signal received by a physical sensor is analog and can have an infinite number of possible values. The signal must be digitalized to store and process with a digital computer/storage, and the total number of possible sample values needs to be limited.</paragraph><figure><image src="https://static.us.edusercontent.com/files/BhcUyBkLexwZyKAUYCOtp6qu" width="132" height="99"/></figure><paragraph>The most straightforward quantization approach is to divide the total range equally into n portions, which is referred to as <italic>linear quantization</italic>. The above diagram is an example of 4-bit linear quantization. In general cases, however, the linear quantization is inefficient (measured by the bit rate). In 1957, Staut Llyod from Bell Lab first introduced an iterative algorithm to find the local optimum to minimize the quantization variance. </paragraph><file url="https://static.us.edusercontent.com/files/Ll0sUaqjqGJpQDCMNseAwVyQ" filename="Least_squares_quantization_in_PCM.pdf"/><paragraph>The essence of the algorithm Lloyd is the same as the Kmeans algorithm we discussed in this course. In many engineering domains, people still refer to the Kmeans as 'Lloyd's algorithm'. (There have been many studies to find the centroids more efficiently, therefore, 'Lloyd's algorithm' is also called 'Naive Kmeans').</paragraph><paragraph>For those unfamiliar with EE/signal processing, let's highlight a few key points in the paper. </paragraph><paragraph>1. $s(t)$ is the true value of the received signal (physical measurement). In the latter part of the paper, you can consider $s(t)$ and $x$ interchangeable by ignoring the EE context.</paragraph><paragraph>2. $\gamma(x)$ is the label function, that maps $s(t)$ to a set of predefined regions ('quanta' in the paper)</paragraph><paragraph>3. $y(x) = q_{\gamma(x)}$ is the quantized value, i.e., the centroid</paragraph><paragraph>3. the algorithm objective is to minimize the noise, defined as <break/>$$N = \mathbb E[(y(x)- x)^2] =\int_{-\infty}^{\infty} (y(x) - x)^2 p(x)dx$$</paragraph><paragraph>In practice, since we only have a limited amount samples, the objective is then replaced with sample variance</paragraph><paragraph>$$N = \sum_i (y(x_i) - x_i)^2$$</paragraph><paragraph>Note: In many literature, the formulation is commonly written in the distribution form. Some of those will explicitly state that using the empirical (sample) mean/variance/etc for the inference (the algorithm procedure), others will omit such fact. Please keep this in mind when reading literature.</paragraph><paragraph>~~~~~~~~~~~~~~~~~~~~~~~~~</paragraph><paragraph>I also attached the discussion with a previous student in this course.</paragraph><paragraph>Q: The lecture mentioned that the k-means result is subject to changes in the initial point selections. Also, the hierarchical cluster may generate a different result than k-means even if we select the same k. When explaining the results, how can we convince the audience the cluster analysis results are stable? I asked this question because if the cluster results are highly volatile and vary for each run, how can we convince the audience the results are reliable?</paragraph><paragraph>A:</paragraph><paragraph>To answer this question, we first understand the motivation of the Kmeans algorithm. In general, the partitioning/clustering problem is NP-hard, i.e., the search space of the global optimum is combinatorially large.</paragraph><paragraph>Let $m$ be the number of available data points, and $K$ be the number of clusters we try to identify. The total possible clustering configuration becomes $C_n^K = \frac{n(n-1)\cdots(n-K+1)}{K!}$</paragraph><paragraph>When the problem scope is small, e.g., $n,K$ are relatively small, we can use brute force instead of Kmeans to find the global optimum. However, in many cases, take the example in the paper, for example, both $n$ and $K$ can be very large. A communication system can easily produce millions or even billions of sampled signals (available data points), and a reasonable number of clusters can go beyond 128 or 256 (7bit/8bit). Subsequently, the brute force computation becomes infeasible.</paragraph><paragraph>Now back to your question. If the audience is knowledgeable enough in math, they will understand the complexity of the problem and will not attempt to ask for a global optimal solution. Given the inherent limitation of the Kmeans algorithm which is sensitive to the initialization, many studies have been proposed for improvements. You can refer to the attached literature for more details.</paragraph><paragraph/><file url="https://static.us.edusercontent.com/files/joycisb4xbf4Fzmb4UJ1MaXr" filename="electronics-09-01295-v2.pdf"/><paragraph>Meanwhile, another common practice is to repeat the algorithm multiple times with different choices/strategies which will also mitigate the issue.</paragraph><paragraph/><paragraph/><paragraph/><paragraph/></document>

