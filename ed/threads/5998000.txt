Thread ID: 5998000
User 1232277 (Parent Post) says:
<document version="2.0"><paragraph>Hello TAs, </paragraph><paragraph>I have hit the wall (wasted 6+ hrs) trying to debug why my mismatch are increasing as oppose to decreasing as k values increase. I would really appreciate if you could point out my issue and guide me in the right direction. </paragraph><paragraph/><paragraph>My approach as a follows:</paragraph><list style="bullet"><list-item><paragraph>remove isolated nodes (filtered nodes) by removing all the rows form nodes.txt that was not referenced in edges two columns. I basically created a set of list with all values from edges and filtered them in nodes which gave me 1224 rows. </paragraph></list-item><list-item><paragraph>updated the indexes in edges.txt by finding the new index reference in the filtered_nodes and updating them in edges. New edges has the same dimension as before, just with updated indexes</paragraph></list-item><list-item><paragraph>calculated the A, D, L , and KMeans using the same code from the demo codes, inside a function so that I can reuse it with different k values. </paragraph></list-item><list-item><paragraph>For each K, looped through the clusters, calculated the min count ('1', '0') based on the true values and calculate the mismatch. </paragraph><list style="bullet"><list-item><paragraph>Here are my cluster sizes, for e.g. for </paragraph></list-item><list-item><paragraph>k2 = (1222, 2) #miss matchL: 0.479</paragraph></list-item><list-item><paragraph>k 5 = (500, 584, 65, 70, 5) # missmatch: 0.69</paragraph></list-item><list-item><paragraph>k 10 = (211,  132, 32,  57, 51,  39, 367, 23, 5, 307) # miss match: 1.36</paragraph></list-item><list-item><paragraph>Overall mismatch are as follow: [0.479, 0.69, 1.36, 2.25, 3.495]</paragraph></list-item></list></list-item></list><paragraph>Can you <bold>please</bold> help point out there I am having the issue? The questions mentioned to constructing the adjacency matrix and make it symmetrical. I just replicated the code chuck from our demo code for calculating A, D, L. </paragraph><paragraph>My code is pasted below. </paragraph><paragraph/><pre>a = import_graph()<break/>og_node = read_blog_name()<break/>edg = a[:, 0].tolist()<break/>edg.extend(a[:, 1].tolist())<break/>iso_node = list(set(edg)) # selecting unique values form edges <break/>print(len(iso_node))<break/>
filtered_nodes = [og_node[i-1] for i in iso_node]
print(len(filtered_nodes))

# formatting the filtered nodes for easy assibility to the true value indexes
node_columns = [line.split('\t') for line in filtered_nodes]
for col in node_columns:
    col[0] = col[0].strip('"')  # Remove quotes from the first column
    col[2] = col[2].strip('"')  # Remove quotes from the third column
    
    # re-indexing teh edges to synch up wiht the updates nodes.
new_i = []
new_j = []

for ind in range(len(a)): # 0 to 19089 indices
    new_i.append(filtered_nodes.index(og_node[a[ind, 0] -1])) # adding updated indexes first col
    new_j.append(filtered_nodes.index(og_node[a[ind, 1] -1])) # adding updated indexes first col
    
# spectral clustering same code as demo
n = len(filtered_nodes)
v = np.ones((a.shape[0], 1)).flatten()
A = sparse.coo_matrix((v, (new_i, new_j)), shape=(n, n))
A = (A + np.transpose(A))
A = sparse.csc_matrix.todense(A) # ## convert to dense matrix
D = np.diag(1/np.sqrt(np.sum(A, axis=1)).A1)
L = D @ A @ D
L = np.array(L) # ## covert to array

# creating a function that getting clusters setup 
def create_cluster(k, L) :  
    #eighenvalue  
    v, x = np.linalg.eig(L)
    idx_sorted = np.argsort(v) # the index of eigenvalue sorted acsending
    x = x[:, idx_sorted[-k:]] # select the k largest eigenvectors
    x = x/np.repeat(np.sqrt(np.sum(x*x, axis=1).reshape(-1, 1)), k, axis=1)
    kmeans = KMeans(n_clusters=k).fit(x.real) 
    idx = kmeans.labels_  
    clusters = [[]for _ in range(k)]  
    print(f"K: {k}")
    for i in range (idx.shape[0]): 
        clusterIndex = idx[i,]
        clusters[clusterIndex].append(i)
    for l in clusters:
        print('cluster size: ', len(l))
    return clusters 
    
# creating function that calculate the mismatch rate
def cal_mismatch(cluster, nodes):
    nodes_0 = np.take(nodes, cluster, axis = 0)    
    #print( "nodes_0 shape : ", nodes_0.shape, nodes_0)
    count_1 = np.count_nonzero(nodes_0[:, 1]=='1')
    print( f'count_1: {count_1}')
    # print('1: ', nodes_0[nodes_0[:,1] =='1'], '\n')
    count_0 = len(nodes_0) - count_1
    print(f'count_0: {count_0}')
    # print('0: ',nodes_0[nodes_0[:,1] =='0'][:30], '\n')
    print(f'total cluster size: {len(nodes_0)}')
    mismatch = min (count_0, count_1)/len(nodes_0)
    print(f'mismatch: {mismatch} \n')
    return mismatch

#Creat function that put all the mismatched nodes together
def get_mismatch (clusters):  
    mismatch_overall = 0
    c=1
    for cluster in clusters:
        print(f'CLUSTER: {c}')
        # print(cluster[:30])
        c += 1
        mismatch_overall += cal_mismatch(cluster, node_columns)
        
    return mismatch_overall
    
#Setting a list of K values and see what their mismatch rate look like
mismatch_list = []
# K = [i for i in range(2,25)]
K = [2,5,10 ,30,50]

for i in K:
    clusters = create_cluster(i, L)
    mismatch_overall = get_mismatch(clusters)
    mismatch_list.append(mismatch_overall)
    print (f'Overall mismatch for K: {i} = {mismatch_list[-1]} \n')

print (mismatch_list)
    
plt. plot (mismatch_list)
print (min (mismatch_list))
index_mis = mismatch_list.index(min(mismatch_list))  
print(index_mis)

</pre></document>

Answer from User 213547:
Your L equation is wrong. 

  Comment from User 1232277:
  Hello Neepa, 

Could you give me a little more hint please. I got ' L = D @ A @ D ' from the Demo code but still struggling to understand what is wrong. I tried L = D - A which also yielded similar results. 

A = sparse.coo_matrix((v, (new_i, new_j)), shape=(n, n))
A = (A + np.transpose(A))
A = sparse.csc_matrix.todense(A) # ## convert to dense matrix

D = np.diag(1/np.sqrt(np.sum(A, axis=1)).A1)
L = D @ A @ D
L = np.array(L) # ## covert to array


Appreciate your time!

