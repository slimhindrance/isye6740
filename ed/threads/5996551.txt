Thread ID: 5996551
User 134876 (Parent Post) says:
<document version="2.0"><paragraph>Please ask all questions related to HW2 Q2 here.</paragraph><list style="bullet"><list-item><paragraph>There are no runtime requirements for this question, however please keep it reasonable (&lt;10min).</paragraph></list-item></list></document>

Comment from User 1232296:
I'm assuming for this question we are allowed to use packages that can perform eigendecomposition? or are we expected to code that part too?

  Reply from User 213547:
  Yes you may use package for eigendecompostion

Comment from User 1316216:
For both the problems themselves and gradescope, I wanted to confirm:

2.1:  Perform PCA "on countries", meaning we reduce the number of food features to 2 principal components.  We will have a scatter plot with points labeled with country names, and submit the code in gradescope as food_pca().

2.2:  Perform PCA "on foods"; reduce the number of countries to 2 principal components.  Scatter plot is labeled with food names, code in gradescope is named countries_pca()

Is that correct?  I think maybe the gradescope method naming is throwing me based on what it seems to expect as an upload.

Thanks!



  Reply from User 215365:
  Your understanding of the PCA procedure and the format of the scatter plots appears correct. I believe you submit food_consumption.py to Gradescope which includes both of the methods, food_pca and  country_pca.  This Post in Ed further describes the Gradescope procedures for HW2.

Comment from User 1133015:
do we need to standardize the data?

  Reply from User 215365:
  The values of this data set are within the same general range, so you will not need to divide by the standard deviation of the feature values to preprocess the data. It will depend on your implementation (also, whether you use packages) if you subtract by some mean, $\mu$.

Comment from User 1512055:
I'm having some trouble to understand exactly how to implement the algorithm. For what I understood from the lecture, the covariance matrix is equivalent to $\frac{1}{m}X X^T$, where $X = [x^1 - \mu, ..., x^m - \mu]$, and by applying the SVD on it we conclude that the eigenvectors of $C$ will be the left singular vectors of $X$ and the eigenvalues will be the squared singular values obtained by the decomposition. With that, we have enough information to perform PCA.

However, in the slide 28 (lecture 4) we have:

"Eigenvectors of ùê∂ corresponds to left singular vectors of ùëã
Find the weight vectors {ùë§ ! , ùë§ " , ‚ãØ , ùë§ 1 } as the ùëü left singular vectors
of the data matrix ùëã (ùëü is the number of principle components)"

With that in mind, I'm confused. Should I perform the SVD on $X$ itself or on $\frac{1}{m}X X^T$?



  Reply from User 672141:
  Eigendecomposition and SVD are two different ways to perform PCA on the data. For the purposes of this question, you will perform eigendecomposition using the covariance matrix C. 

About SVD, the slide mentions the input to be X but through derivation you can connect it to C= $\frac{1}{m}XX^T$ .



Comment from User 1324673:
I'm having trouble understanding this part of the lecture. Are we supposed to do this for output for question 2? It's looking better to me when I plot the w vectors instead of the z vectors. And what about for question 3?



  Reply from User 1497720:
  The w vectors are the principal vectors/eigenvectors onto which we intend to project each original datapoint xi. We normalize them by dividing them by the sqrt(lamda). The mean-centered xi is xi - mu. To find the projection of our mean centred datapoints onto the principal vectors we do the above dot products. Its like if you had a datapoint in x,y plane lets say v = [5,2] to find its projection on the x-axis i.e. the [1,0] vector we dot them to get [5,0] which makes sense since 5 is the x-coordinate of v. In that sense instead of the x-coordinate of v we want to find the wi-coordinate of v where v = xi - mu (and wi are orthonormal to each other just how x,y and z axis are) 

