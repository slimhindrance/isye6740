Thread ID: 5982431
User 1128042 (Parent Post) says:
<document version="2.0"><paragraph>I understand why k-means clustering is a non-convex optimization problem, but I am struggling to follow the reasoning in lecture 2 where Dr. Xie uses the generalized form of k-means to turn it into a convex optimization problem (in many cases).</paragraph><paragraph>The only change that I see is using the dissimilarity function in place of the l2 norm. But how does this bring about convexity? Especially if the dissimilarity function that I choose to calculate distance is the euclidean distance, then we're right back where we started.</paragraph><paragraph>Any further reading or guidance on this would be appreciated!</paragraph><paragraph/></document>

Answer from User 300188:
If you're referring to slide 29 in the lecture 2 slides, what's being said here is that most traditional distance metrics result in convex optimization problems because their cost functions typically have a global minimum. So if you choose $d(x^i, v)^2$ to be some type of traditional distance metric then more often than not the problem will become convex. Since this choice of $d(x^i, v)^2$ is user dependent, the way to prove convexity will change slightly each time. It's possible to choose $d(x^i, v)^2$ in a manner such that the problem isn't necessarily convex. 





