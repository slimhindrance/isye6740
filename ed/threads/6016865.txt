Thread ID: 6016865
User 497513 (Parent Post) says:
<document version="2.0"><paragraph>The term 'compression' makes sense if we look at the problem from the information theory perspective: </paragraph><paragraph>In the original image, each pixel is represented with a 3x8bit pixel, so the information within each pixel is 24bit. The dictionary (possible choices of the pixel values) is 3x256. Furthermore, the explicit definition of information conveyed by each pixel is measured/defined as the entropy</paragraph><paragraph>$$H(S) = -\sum_{s\in S} P(s)\log_c P(s)$$</paragraph><paragraph>where $S$ is the random variable of the pixel outcome, and $c$ is the base. With a different choice of $c$, there would be a different unit, for example, for $c=2$, it is called one bit, for $c = e$, it is called one nit or nat<break/><link href="https://en.wikipedia.org/wiki/Units_of_information">https://en.wikipedia.org/wiki/Units_of_information</link>. </paragraph><paragraph>So for our image example, assume all possible values of each color channel have an equal probability, each pixel provides</paragraph><paragraph>$$H(S) = - 3 \times \sum_{256} \frac1{256} \log_2 \frac1{256} = -3 \times 256 \times \frac1{256} \times (-8) = 24 \text{bit}$$, When we perform the clustering, we essentially reduce the dictionary size from 256 down to the choice of k, say k=2 for example.</paragraph><paragraph>In this way, each pixel has only two possible values. Following the above definition, now each pixel can only provide</paragraph><paragraph>$$H(S) = -   \sum_{2} \frac1{2} \log_2 \frac1{2} = 1\text{bit}$$</paragraph><paragraph/><paragraph>Assume we have 1 million pixels, the original image will have 24M bits. For the common computer system, such an image will take 24M/24 = 1M Byte space. Imagine we have a much larger size of data, the difference will be significant.</paragraph><paragraph>Lastly, if you save the kmeans output with the BMP format, the file size will remain the same. This is because the computer system does not incorporate the new coding system. When saved to a file, each pixel (assume we save them as unit8) will still take 24bit space. </paragraph><paragraph>However, if you save it in jpeg format, the file size would very likely be much smaller. Because jpeg, as a lossy coding system, performs <bold>additional compression</bold> on top of your kmean output to remove the statistical redundancy from the image.</paragraph><paragraph>~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</paragraph><paragraph><bold>ps:</bold> </paragraph><paragraph>The compression we've done in this hw example only reduces the number of the choice of possible values, this belongs to the so-called 'quantization' in signal processing studies. The more in-depth compression falls into the category called 'coding' (including both lossy and lossless).</paragraph><paragraph>A real-world compression tool usually consists of several elements. Take JPEG, the most commonly used image compression tool, for example, a raw image will go through the following procedure: </paragraph><paragraph>1. convert the image representation (usually RGB) to $YC_BC_R$</paragraph><paragraph>2. divide the image into patches and perform Discrete Fourier Transform (DCT).</paragraph><paragraph>3. perform quantization (supposedly, one can use Kmeans here) to the DCT coefficients. </paragraph><paragraph>(the idea behind step 2&amp;3 is very similar to the dimensionality reduction with PCA: find an orthogonal representation, keep only the ones with greater information (large variance, low frequency), and discard the rest.)</paragraph><paragraph>4. further compress the result from 3 with entropy encoding (lossless coding algorithm). </paragraph></document>

