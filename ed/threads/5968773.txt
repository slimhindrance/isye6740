Thread ID: 5968773
User 1128019 (Parent Post) says:
<document version="2.0"><paragraph>Hi TAs,</paragraph><paragraph>Regarding question 4, I ran into the following issues with my code but I haven't been able to identify what is causing them:</paragraph><paragraph>Issue #1: My understanding is that the overall mismatch rate should decrease monotonically as k increases. However, when I tested k = [2, 5, 10, 30, 50], I noticed that my mismatch rate first dips a bit and then starts going up.</paragraph><paragraph>Issue #2: When I tested with only k = 2, since there are two true classes (0 and 1s), I was expecting that the algorithm should do a fairly good job at classifying nodes with true labels of 0s vs 1s. However, when I printed out the nodes that were classified into the same cluster along with their true labels, I noticed that my algorithm grouped all the 0s and 1s together.</paragraph><paragraph>What I have tried:</paragraph><paragraph>Attempt #1: Print out the dimension of updated adjacency matrix and confirmed that its dimension is (1224, 1224), which validates that nodes with no connections have been removed.</paragraph><paragraph>Attempt #2: My original method uses the algorithm demonstrated in the test_football demo code where the professor uses L = D @ A @ D, sort the eigenvalues from smallest to largest, and index the eigenvectors associated with k largest eigenvalues. Since this method is not working for me, I tried the second approach demonstrated in the test_tworings file, which computes L = D - A, and selects the eigenvectors associated with k smallest eigenvalues. For some reason, this approach gave me even larger overall mismatch rate. So I have to revert back to method #1.</paragraph><paragraph>I also checked and printed out multiple variables such as making sure that eigenvalues and eigenvectors appear reasonable, the eigenvalues is sorted properly, the computation process aligns with the method illustrated in test_football file and that I understand the computation steps.</paragraph><paragraph>Please note that one minor deviation from the test_football implementation is that I computed the eigenvalues and eigenvectors using scipy package, as for some reason whenever I tried np.linalg.eig() function, my eigenvectors are always nan values. The same happened when I tried to run test_football code and had to replace np eig function with scipy's, it then ran properly as the results match with the <link href="https://github.gatech.edu/pages/ISYE6740/CDA-handbook/week3/football_demo.html">git</link>.</paragraph><paragraph>For your reference, below is my method #1 (L = D @ A @ D) implementation:</paragraph><pre>def spectral_clusters(k):
    # The following codes are adapted from test_football demo code.
    i = edges[:, 0] - 1  # Take the first column and decrement by 1
    j = edges[:, 1] - 1  # Take the second column and decrement by 1
    n = len(nodes)

    v = np.ones(edges.shape[0])

    # Create adjacency matrix
    A = sparse.coo_matrix((v, (i, j)), shape=(n, n))
    A = A + A.T
    A = sparse.csc_matrix.todense(A)  # Convert to dense matrix

    # Remove nodes that have no connections
    degree = np.sum(np.array(A), axis=0).flatten()
    connected_nodes = degree &gt; 0

    updated_edges = A[connected_nodes][:, connected_nodes]

    D = np.diag(1 / np.sqrt(np.sum(updated_edges, axis=1)).A1)
    L = D @ updated_edges @ D
    L = np.array(L)  # ## covert to array
    # print(f"L is {L}")

    # The following code is adapted from test_football demo code.
    # Eigendecomposition
    v, x = scipy.linalg.eigh(L)

    idx_sorted = np.argsort(v)

    # Obtain the largest eigenvectors
    x = x[:, idx_sorted[-k:]]
    x = x / np.repeat(np.sqrt(np.sum(x * x, axis=1).reshape(-1, 1)), k, axis=1)

    kmeans = KMeans(n_clusters=k).fit(x)
    c_idx = kmeans.labels_

    return (c_idx, len(updated_edges))
</pre><paragraph>If you could provide any insight of what might have gone wrong, it would be very much appreciated!</paragraph><paragraph>Thank you!</paragraph><paragraph/><paragraph/></document>

Answer from User 213547:
What are your mismatch rates?



  Comment from User 1128019:
  Hi Neepa, for k = [2, 5, 10, 30, 50], my mismatch rates = array([0.3824, 0.1732, 0.1773, 0.2949, 0.4265]). The lowest that I was able to get is when k = 6 (Question part 2 where I tested with multiple k values), but still my mismatch rate is hanging around 0.17. I wasn't able to get it below 10% as you mentioned in the office hour. I then realized that something is wrong with my algorithm.

